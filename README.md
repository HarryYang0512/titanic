# ğŸ›³ï¸ Titanic - Machine Learning from Disaster

This project tackles the iconic **Titanic Kaggle competition**, where the goal is to predict which passengers survived the Titanic shipwreck using a combination of demographic and socio-economic data.

The project follows a complete ML pipeline:
- ğŸ“Š Exploratory Data Analysis (EDA)
- ğŸ› ï¸ Feature Engineering
- ğŸ¤– Model Training (Logistic Regression, Random Forest, XGBoost, LightGBM)
- ğŸ§ª Hyperparameter Tuning
- ğŸ¤ Ensemble Learning
- ğŸ“ˆ Submission to Kaggle and performance comparison

---

## ğŸ“ Project Structure
```
ğŸ“¦ Titanic-ML
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ train_cleaned.csv
â”œâ”€â”€ test_cleaned.csv
â”œâ”€â”€ titanic_model_pipeline.ipynb
â”œâ”€â”€ submission/
â”‚   â””â”€â”€ each model submission.csv
â””â”€â”€ README.md
```

---

## ğŸ§  Best Model (So Far)

> âœ… **Logistic Regression + Feature Engineered Data**  
> ğŸ† **Kaggle Score: 0.78229**

Despite being a simple model, logistic regression outperformed many complex ones when combined with thoughtful feature engineering.

---

## ğŸ“Š Model Performance Comparison

| Model                    | Dataset        | Kaggle Score |
|-------------------------|----------------|--------------|
| **Logistic Regression** | Engineered     | **0.78229** âœ… |
| Ensemble (Voting)       | Engineered     | 0.77990      |
| XGBoost                 | Engineered     | 0.77033      |
| XGBoost                 | Original       | 0.77272      |
| LightGBM (GridSearch)   | Original       | 0.77272      |
| Random Forest (Optimized)| Engineered    | 0.76555      |
| LightGBM (GridSearch)   | Engineered     | 0.76315      |
| Random Forest           | Engineered     | 0.74880      |
| Random Forest           | Original       | 0.74641      |
| LightGBM (Simple)       | Original       | 0.74641      |
| LightGBM (Simple)       | Engineered     | 0.74401      |

---

## ğŸ› ï¸ Feature Engineering Highlights

- `FamilySize` = `SibSp` + `Parch` + 1
- `IsAlone` = 1 if `FamilySize == 1`, else 0
- Extracted `Title` from `Name`
- One-hot encoding on `Sex`, `Embarked`, and `Title`

---

## ğŸ“¤ Submissions

All model predictions are saved in the `submission/` folder for reproducibility and score tracking.

---

## ğŸ“Œ Next Steps

- Visualize feature importance and SHAP values
- Build a stacking ensemble
- Try interaction features (polynomial terms)
- Move on to the next ML project in the [YouTube video roadmap](https://www.youtube.com/watch?v=QlbyGPVaRSE)

---

## ğŸ™Œ Credits

This project is inspired by the video:  
**"22 Machine Learning Projects That Will Make You A God At Data Science"**  
ğŸ”— [Watch it here](https://www.youtube.com/watch?v=QlbyGPVaRSE)
